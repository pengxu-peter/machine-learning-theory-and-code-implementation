
$\sum_{i=0}^n i^2 = \frac{(n^2+n)(2n+1)}{6}$


$$\sum_{i=0}^n i^2 = \frac{(n^2+n)(2n+1)}{6}$$


$\alpha$

$\beta$

$\omega$

$\Gamma$

$$ \sum_{i=1}^{m} a_i = \sum_{i=1}^{m} (t_i + \Phi_i - \Phi_{i-1}) = \left( \sum_{i=1}^m t_i \right) + \Phi_m - \Phi_0 $$

$$
L =  \underbrace{ \frac{1}{N} \sum_i L_i }_\text{data loss} +  
 \underbrace{ \lambda R(W) }_\text{regularization loss} 
$$


$$ 
L =  \underbrace{ \frac{1}{N} \sum_i L_i }\_\text{data loss} +  
 \underbrace{ \lambda R(W) }\_\text{regularization loss} 
$$

$$
ax^{2} + by^{2} + c = 0
$$

$$
\sigma=\sqrt{\frac{1}{n}{\sum_{k=1}^n(x_i-\bar{x})^2}}
$$

![](http://latex.codecogs.com/gif.latex?%5Csigma%3D%5Csqrt%7B%5Cfrac%7B1%7D%7Bn%7D%7B%5Csum_%7Bk%3D1%7D%5En%28x_i-%5Cbar%7Bx%7D%29%5E2%7D%7D)

<script type="text/javascript" 
        src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"> 
</script>

<script type="text/x-mathjax-config"> 
        MathJax.Hub.Config({ 
            tex2jax: {
                inlineMath: [['$','$'], ['\(','\)']]
            }
        });
</script>

